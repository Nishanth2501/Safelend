{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SafeLend - Feature Engineering Checks\n",
        "\n",
        "This notebook contains feature engineering validation and checks for the SafeLend project.\n",
        "\n",
        "## Overview\n",
        "- Feature engineering validation\n",
        "- Data quality checks\n",
        "- Feature distribution comparisons\n",
        "- Model feature importance analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load processed data\n",
        "print(\"Loading processed data...\")\n",
        "train_df = pd.read_parquet('../Data/processed/train_modeling.parquet')\n",
        "test_df = pd.read_parquet('../Data/processed/test_modeling.parquet')\n",
        "\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "\n",
        "# Check if TARGET column exists\n",
        "if 'TARGET' in train_df.columns:\n",
        "    print(f\"Target distribution: {train_df['TARGET'].value_counts().to_dict()}\")\n",
        "else:\n",
        "    print(\"No TARGET column found in train data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature quality checks\n",
        "print(\"=== FEATURE QUALITY CHECKS ===\")\n",
        "\n",
        "# Check for constant features\n",
        "constant_features = []\n",
        "for col in train_df.columns:\n",
        "    if col != 'TARGET' and train_df[col].nunique() <= 1:\n",
        "        constant_features.append(col)\n",
        "\n",
        "print(f\"Constant features found: {len(constant_features)}\")\n",
        "if constant_features:\n",
        "    print(f\"Constant features: {constant_features}\")\n",
        "\n",
        "# Check for near-constant features (99% same value)\n",
        "near_constant_features = []\n",
        "for col in train_df.columns:\n",
        "    if col != 'TARGET':\n",
        "        value_counts = train_df[col].value_counts()\n",
        "        max_freq = value_counts.iloc[0] / len(train_df)\n",
        "        if max_freq > 0.99:\n",
        "            near_constant_features.append(col)\n",
        "\n",
        "print(f\"Near-constant features (>99% same value): {len(near_constant_features)}\")\n",
        "if near_constant_features:\n",
        "    print(f\"Near-constant features: {near_constant_features}\")\n",
        "\n",
        "# Check for features with high missing values\n",
        "missing_threshold = 0.5  # 50%\n",
        "high_missing_features = []\n",
        "for col in train_df.columns:\n",
        "    if col != 'TARGET':\n",
        "        missing_pct = train_df[col].isnull().mean()\n",
        "        if missing_pct > missing_threshold:\n",
        "            high_missing_features.append((col, missing_pct))\n",
        "\n",
        "print(f\"Features with >{missing_threshold*100}% missing values: {len(high_missing_features)}\")\n",
        "if high_missing_features:\n",
        "    print(\"High missing features:\")\n",
        "    for col, pct in high_missing_features:\n",
        "        print(f\"  {col}: {pct:.2%}\")\n",
        "\n",
        "# Feature variance analysis\n",
        "numeric_features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'TARGET' in numeric_features:\n",
        "    numeric_features.remove('TARGET')\n",
        "\n",
        "feature_variance = train_df[numeric_features].var().sort_values(ascending=True)\n",
        "low_variance_features = feature_variance[feature_variance < 1e-6]\n",
        "\n",
        "print(f\"Features with very low variance (< 1e-6): {len(low_variance_features)}\")\n",
        "if len(low_variance_features) > 0:\n",
        "    print(\"Low variance features:\")\n",
        "    for col, var in low_variance_features.items():\n",
        "        print(f\"  {col}: {var:.2e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data leakage checks\n",
        "print(\"=== DATA LEAKAGE CHECKS ===\")\n",
        "\n",
        "# Check for features with perfect correlation with target\n",
        "if 'TARGET' in train_df.columns:\n",
        "    target_corr = train_df[numeric_features + ['TARGET']].corr()['TARGET'].drop('TARGET')\n",
        "    perfect_corr = target_corr[abs(target_corr) > 0.99]\n",
        "    \n",
        "    print(f\"Features with perfect correlation with target: {len(perfect_corr)}\")\n",
        "    if len(perfect_corr) > 0:\n",
        "        print(\"Perfect correlation features:\")\n",
        "        for col, corr in perfect_corr.items():\n",
        "            print(f\"  {col}: {corr:.4f}\")\n",
        "\n",
        "# Check for features that are too predictive (potential leakage)\n",
        "high_corr_features = target_corr[abs(target_corr) > 0.8]\n",
        "print(f\"Features with very high correlation (>0.8): {len(high_corr_features)}\")\n",
        "if len(high_corr_features) > 0:\n",
        "    print(\"High correlation features:\")\n",
        "    for col, corr in high_corr_features.items():\n",
        "        print(f\"  {col}: {corr:.4f}\")\n",
        "\n",
        "# Check for features with unrealistic distributions\n",
        "print(\"\\n=== UNREALISTIC DISTRIBUTION CHECKS ===\")\n",
        "\n",
        "# Check for features with extreme values\n",
        "extreme_features = []\n",
        "for col in numeric_features:\n",
        "    if col in train_df.columns:\n",
        "        q99 = train_df[col].quantile(0.99)\n",
        "        q01 = train_df[col].quantile(0.01)\n",
        "        max_val = train_df[col].max()\n",
        "        min_val = train_df[col].min()\n",
        "        \n",
        "        # Check for extreme outliers\n",
        "        if max_val > q99 * 10 or min_val < q01 * 10:\n",
        "            extreme_features.append((col, min_val, max_val, q01, q99))\n",
        "\n",
        "print(f\"Features with extreme outliers: {len(extreme_features)}\")\n",
        "if extreme_features:\n",
        "    print(\"Extreme outlier features:\")\n",
        "    for col, min_val, max_val, q01, q99 in extreme_features:\n",
        "        print(f\"  {col}: min={min_val:.2f}, max={max_val:.2f}, q01={q01:.2f}, q99={q99:.2f}\")\n",
        "\n",
        "# Check for features with suspicious patterns\n",
        "print(\"\\n=== SUSPICIOUS PATTERN CHECKS ===\")\n",
        "\n",
        "# Check for features that are identical between train and test\n",
        "identical_features = []\n",
        "for col in train_df.columns:\n",
        "    if col != 'TARGET' and col in test_df.columns:\n",
        "        train_unique = set(train_df[col].dropna().unique())\n",
        "        test_unique = set(test_df[col].dropna().unique())\n",
        "        if train_unique == test_unique:\n",
        "            identical_features.append(col)\n",
        "\n",
        "print(f\"Features identical between train and test: {len(identical_features)}\")\n",
        "if identical_features:\n",
        "    print(f\"Identical features: {identical_features}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature distribution comparison\n",
        "print(\"=== FEATURE DISTRIBUTION COMPARISON ===\")\n",
        "\n",
        "# Compare distributions between train and test\n",
        "common_features = [col for col in train_df.columns if col != 'TARGET' and col in test_df.columns]\n",
        "print(f\"Comparing {len(common_features)} common features between train and test\")\n",
        "\n",
        "# Statistical tests for distribution differences\n",
        "from scipy import stats\n",
        "\n",
        "significant_differences = []\n",
        "for col in common_features[:20]:  # Check first 20 features to avoid overwhelming output\n",
        "    if col in numeric_features:\n",
        "        train_vals = train_df[col].dropna()\n",
        "        test_vals = test_df[col].dropna()\n",
        "        \n",
        "        if len(train_vals) > 100 and len(test_vals) > 100:  # Sufficient samples\n",
        "            # Kolmogorov-Smirnov test\n",
        "            ks_stat, ks_pvalue = stats.ks_2samp(train_vals, test_vals)\n",
        "            \n",
        "            # Mann-Whitney U test\n",
        "            mw_stat, mw_pvalue = stats.mannwhitneyu(train_vals, test_vals, alternative='two-sided')\n",
        "            \n",
        "            if ks_pvalue < 0.05 or mw_pvalue < 0.05:\n",
        "                significant_differences.append({\n",
        "                    'feature': col,\n",
        "                    'ks_pvalue': ks_pvalue,\n",
        "                    'mw_pvalue': mw_pvalue,\n",
        "                    'train_mean': train_vals.mean(),\n",
        "                    'test_mean': test_vals.mean()\n",
        "                })\n",
        "\n",
        "print(f\"Features with significant distribution differences: {len(significant_differences)}\")\n",
        "if significant_differences:\n",
        "    print(\"Significant differences (p < 0.05):\")\n",
        "    for diff in significant_differences:\n",
        "        print(f\"  {diff['feature']}: KS p={diff['ks_pvalue']:.4f}, MW p={diff['mw_pvalue']:.4f}\")\n",
        "        print(f\"    Train mean: {diff['train_mean']:.4f}, Test mean: {diff['test_mean']:.4f}\")\n",
        "\n",
        "# Visualize distribution differences for key features\n",
        "key_features = ['AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_ANNUITY']\n",
        "key_features = [f for f in key_features if f in common_features]\n",
        "\n",
        "if key_features:\n",
        "    plt.figure(figsize=(15, 5 * len(key_features)))\n",
        "    \n",
        "    for i, feature in enumerate(key_features):\n",
        "        plt.subplot(len(key_features), 2, 2*i + 1)\n",
        "        plt.hist(train_df[feature].dropna(), bins=50, alpha=0.7, label='Train', density=True)\n",
        "        plt.hist(test_df[feature].dropna(), bins=50, alpha=0.7, label='Test', density=True)\n",
        "        plt.title(f'{feature} Distribution Comparison')\n",
        "        plt.legend()\n",
        "        plt.yscale('log')\n",
        "        \n",
        "        plt.subplot(len(key_features), 2, 2*i + 2)\n",
        "        train_q = np.percentile(train_df[feature].dropna(), range(0, 101))\n",
        "        test_q = np.percentile(test_df[feature].dropna(), range(0, 101))\n",
        "        plt.plot(train_q, test_q, 'o', markersize=2)\n",
        "        plt.plot([train_q.min(), train_q.max()], [train_q.min(), train_q.max()], 'r--')\n",
        "        plt.title(f'{feature} Q-Q Plot')\n",
        "        plt.xlabel('Train Quantiles')\n",
        "        plt.ylabel('Test Quantiles')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extreme values check\n",
        "print(\"=== EXTREME VALUES CHECK ===\")\n",
        "\n",
        "# Check for extreme outliers using IQR method\n",
        "extreme_outliers = {}\n",
        "for col in numeric_features[:20]:  # Check first 20 features\n",
        "    if col in train_df.columns:\n",
        "        Q1 = train_df[col].quantile(0.25)\n",
        "        Q3 = train_df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        outliers = train_df[(train_df[col] < lower_bound) | (train_df[col] > upper_bound)]\n",
        "        outlier_pct = len(outliers) / len(train_df) * 100\n",
        "        \n",
        "        if outlier_pct > 5:  # More than 5% outliers\n",
        "            extreme_outliers[col] = {\n",
        "                'outlier_count': len(outliers),\n",
        "                'outlier_percentage': outlier_pct,\n",
        "                'lower_bound': lower_bound,\n",
        "                'upper_bound': upper_bound\n",
        "            }\n",
        "\n",
        "print(f\"Features with >5% extreme outliers: {len(extreme_outliers)}\")\n",
        "if extreme_outliers:\n",
        "    print(\"Extreme outlier features:\")\n",
        "    for col, info in extreme_outliers.items():\n",
        "        print(f\"  {col}: {info['outlier_count']} outliers ({info['outlier_percentage']:.1f}%)\")\n",
        "\n",
        "# Check for infinite values\n",
        "infinite_features = []\n",
        "for col in numeric_features:\n",
        "    if col in train_df.columns:\n",
        "        if np.isinf(train_df[col]).any():\n",
        "            infinite_count = np.isinf(train_df[col]).sum()\n",
        "            infinite_features.append((col, infinite_count))\n",
        "\n",
        "print(f\"\\nFeatures with infinite values: {len(infinite_features)}\")\n",
        "if infinite_features:\n",
        "    print(\"Infinite value features:\")\n",
        "    for col, count in infinite_features:\n",
        "        print(f\"  {col}: {count} infinite values\")\n",
        "\n",
        "# Check for features with suspicious ranges\n",
        "suspicious_ranges = []\n",
        "for col in numeric_features[:20]:\n",
        "    if col in train_df.columns:\n",
        "        min_val = train_df[col].min()\n",
        "        max_val = train_df[col].max()\n",
        "        \n",
        "        # Check for suspicious ranges\n",
        "        if min_val < -1e6 or max_val > 1e6:\n",
        "            suspicious_ranges.append((col, min_val, max_val))\n",
        "\n",
        "print(f\"\\nFeatures with suspicious ranges: {len(suspicious_ranges)}\")\n",
        "if suspicious_ranges:\n",
        "    print(\"Suspicious range features:\")\n",
        "    for col, min_val, max_val in suspicious_ranges:\n",
        "        print(f\"  {col}: [{min_val:.2e}, {max_val:.2e}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature correlation analysis\n",
        "print(\"=== FEATURE CORRELATION ANALYSIS ===\")\n",
        "\n",
        "# Calculate correlation matrix for numeric features\n",
        "correlation_matrix = train_df[numeric_features].corr()\n",
        "\n",
        "# Find highly correlated feature pairs\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_val = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_val) > 0.8:  # High correlation threshold\n",
        "            high_corr_pairs.append({\n",
        "                'feature1': correlation_matrix.columns[i],\n",
        "                'feature2': correlation_matrix.columns[j],\n",
        "                'correlation': corr_val\n",
        "            })\n",
        "\n",
        "print(f\"Highly correlated feature pairs (|correlation| > 0.8): {len(high_corr_pairs)}\")\n",
        "if high_corr_pairs:\n",
        "    print(\"High correlation pairs:\")\n",
        "    for pair in high_corr_pairs:\n",
        "        print(f\"  {pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.4f}\")\n",
        "\n",
        "# Visualize correlation matrix for top features\n",
        "if 'TARGET' in train_df.columns:\n",
        "    target_corr = train_df[numeric_features + ['TARGET']].corr()['TARGET'].drop('TARGET')\n",
        "    top_features = target_corr.abs().sort_values(ascending=False).head(15).index.tolist()\n",
        "    \n",
        "    if len(top_features) > 0:\n",
        "        corr_subset = correlation_matrix.loc[top_features, top_features]\n",
        "        \n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(corr_subset, annot=True, cmap='coolwarm', center=0, \n",
        "                   square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
        "        plt.title('Correlation Matrix - Top Features by Target Correlation')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Multicollinearity analysis using VIF (Variance Inflation Factor)\n",
        "print(\"\\n=== MULTICOLLINEARITY ANALYSIS ===\")\n",
        "\n",
        "# Calculate VIF for top features (simplified version)\n",
        "def calculate_vif(df, features):\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    \n",
        "    vif_data = []\n",
        "    for feature in features[:10]:  # Limit to first 10 features for performance\n",
        "        X = df[features].drop(columns=[feature])\n",
        "        y = df[feature]\n",
        "        \n",
        "        # Handle missing values\n",
        "        mask = ~(X.isnull().any(axis=1) | y.isnull())\n",
        "        X_clean = X[mask]\n",
        "        y_clean = y[mask]\n",
        "        \n",
        "        if len(X_clean) > 100:  # Sufficient samples\n",
        "            model = LinearRegression().fit(X_clean, y_clean)\n",
        "            r_squared = model.score(X_clean, y_clean)\n",
        "            \n",
        "            if r_squared < 0.999:  # Avoid division by zero\n",
        "                vif = 1 / (1 - r_squared)\n",
        "                vif_data.append({'feature': feature, 'vif': vif})\n",
        "    \n",
        "    return pd.DataFrame(vif_data)\n",
        "\n",
        "vif_df = calculate_vif(train_df, numeric_features)\n",
        "if len(vif_df) > 0:\n",
        "    high_vif = vif_df[vif_df['vif'] > 5]  # VIF > 5 indicates multicollinearity\n",
        "    print(f\"Features with high VIF (>5): {len(high_vif)}\")\n",
        "    if len(high_vif) > 0:\n",
        "        print(\"High VIF features:\")\n",
        "        for _, row in high_vif.iterrows():\n",
        "            print(f\"  {row['feature']}: VIF = {row['vif']:.2f}\")\n",
        "    else:\n",
        "        print(\"No features with high VIF detected\")\n",
        "else:\n",
        "    print(\"VIF calculation not possible (insufficient data or perfect correlation)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature stability check\n",
        "print(\"=== FEATURE STABILITY CHECK ===\")\n",
        "\n",
        "# Check feature stability over different time periods (if applicable)\n",
        "# For this analysis, we'll check stability between train and test sets\n",
        "\n",
        "stability_results = []\n",
        "for col in common_features[:20]:  # Check first 20 features\n",
        "    if col in numeric_features:\n",
        "        train_vals = train_df[col].dropna()\n",
        "        test_vals = test_df[col].dropna()\n",
        "        \n",
        "        if len(train_vals) > 100 and len(test_vals) > 100:\n",
        "            # Calculate stability metrics\n",
        "            train_mean = train_vals.mean()\n",
        "            test_mean = test_vals.mean()\n",
        "            train_std = train_vals.std()\n",
        "            test_std = test_vals.std()\n",
        "            \n",
        "            # Population Stability Index (PSI)\n",
        "            # Simplified PSI calculation\n",
        "            train_hist, train_bins = np.histogram(train_vals, bins=10)\n",
        "            test_hist, _ = np.histogram(test_vals, bins=train_bins)\n",
        "            \n",
        "            train_pct = train_hist / len(train_vals)\n",
        "            test_pct = test_hist / len(test_vals)\n",
        "            \n",
        "            # Avoid division by zero\n",
        "            train_pct = np.where(train_pct == 0, 1e-6, train_pct)\n",
        "            test_pct = np.where(test_pct == 0, 1e-6, test_pct)\n",
        "            \n",
        "            psi = np.sum((train_pct - test_pct) * np.log(train_pct / test_pct))\n",
        "            \n",
        "            stability_results.append({\n",
        "                'feature': col,\n",
        "                'train_mean': train_mean,\n",
        "                'test_mean': test_mean,\n",
        "                'mean_diff_pct': abs(train_mean - test_mean) / train_mean * 100 if train_mean != 0 else 0,\n",
        "                'train_std': train_std,\n",
        "                'test_std': test_std,\n",
        "                'std_diff_pct': abs(train_std - test_std) / train_std * 100 if train_std != 0 else 0,\n",
        "                'psi': psi\n",
        "            })\n",
        "\n",
        "# Analyze stability results\n",
        "if stability_results:\n",
        "    stability_df = pd.DataFrame(stability_results)\n",
        "    \n",
        "    # Features with high PSI (>0.2 indicates significant shift)\n",
        "    high_psi = stability_df[stability_df['psi'] > 0.2]\n",
        "    print(f\"Features with high PSI (>0.2): {len(high_psi)}\")\n",
        "    if len(high_psi) > 0:\n",
        "        print(\"High PSI features:\")\n",
        "        for _, row in high_psi.iterrows():\n",
        "            print(f\"  {row['feature']}: PSI = {row['psi']:.4f}\")\n",
        "    \n",
        "    # Features with significant mean shift (>10%)\n",
        "    high_mean_shift = stability_df[stability_df['mean_diff_pct'] > 10]\n",
        "    print(f\"\\nFeatures with significant mean shift (>10%): {len(high_mean_shift)}\")\n",
        "    if len(high_mean_shift) > 0:\n",
        "        print(\"High mean shift features:\")\n",
        "        for _, row in high_mean_shift.iterrows():\n",
        "            print(f\"  {row['feature']}: Mean diff = {row['mean_diff_pct']:.1f}%\")\n",
        "    \n",
        "    # Visualize stability metrics\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.scatter(stability_df['psi'], stability_df['mean_diff_pct'])\n",
        "    plt.xlabel('PSI')\n",
        "    plt.ylabel('Mean Difference (%)')\n",
        "    plt.title('PSI vs Mean Difference')\n",
        "    plt.axhline(y=10, color='r', linestyle='--', alpha=0.5)\n",
        "    plt.axvline(x=0.2, color='r', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    plt.subplot(1, 3, 2)\n",
        "    stability_df['psi'].hist(bins=20)\n",
        "    plt.xlabel('PSI')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('PSI Distribution')\n",
        "    \n",
        "    plt.subplot(1, 3, 3)\n",
        "    stability_df['mean_diff_pct'].hist(bins=20)\n",
        "    plt.xlabel('Mean Difference (%)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Mean Difference Distribution')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No stability analysis possible (insufficient data)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature engineering summary\n",
        "print(\"=== FEATURE ENGINEERING SUMMARY ===\")\n",
        "\n",
        "# Compile all findings\n",
        "summary = {\n",
        "    'total_features': len(train_df.columns) - 1,  # Exclude TARGET\n",
        "    'constant_features': len(constant_features),\n",
        "    'near_constant_features': len(near_constant_features),\n",
        "    'high_missing_features': len(high_missing_features),\n",
        "    'low_variance_features': len(low_variance_features),\n",
        "    'perfect_corr_features': len(perfect_corr) if 'TARGET' in train_df.columns else 0,\n",
        "    'high_corr_pairs': len(high_corr_pairs),\n",
        "    'extreme_outlier_features': len(extreme_outliers),\n",
        "    'infinite_value_features': len(infinite_features),\n",
        "    'suspicious_range_features': len(suspicious_ranges),\n",
        "    'identical_features': len(identical_features),\n",
        "    'significant_dist_diff': len(significant_differences),\n",
        "    'high_psi_features': len(high_psi) if 'high_psi' in locals() else 0,\n",
        "    'high_vif_features': len(high_vif) if 'high_vif' in locals() and len(vif_df) > 0 else 0\n",
        "}\n",
        "\n",
        "print(\"FEATURE QUALITY SUMMARY:\")\n",
        "print(f\"  Total features analyzed: {summary['total_features']}\")\n",
        "print(f\"  Constant features: {summary['constant_features']}\")\n",
        "print(f\"  Near-constant features: {summary['near_constant_features']}\")\n",
        "print(f\"  High missing value features: {summary['high_missing_features']}\")\n",
        "print(f\"  Low variance features: {summary['low_variance_features']}\")\n",
        "print(f\"  Perfect correlation with target: {summary['perfect_corr_features']}\")\n",
        "print(f\"  High correlation pairs: {summary['high_corr_pairs']}\")\n",
        "print(f\"  Extreme outlier features: {summary['extreme_outlier_features']}\")\n",
        "print(f\"  Infinite value features: {summary['infinite_value_features']}\")\n",
        "print(f\"  Suspicious range features: {summary['suspicious_range_features']}\")\n",
        "print(f\"  Identical train/test features: {summary['identical_features']}\")\n",
        "print(f\"  Significant distribution differences: {summary['significant_dist_diff']}\")\n",
        "print(f\"  High PSI features: {summary['high_psi_features']}\")\n",
        "print(f\"  High VIF features: {summary['high_vif_features']}\")\n",
        "\n",
        "# Recommendations\n",
        "print(\"\\nRECOMMENDATIONS:\")\n",
        "print(\"1. FEATURE REMOVAL:\")\n",
        "if summary['constant_features'] > 0:\n",
        "    print(f\"   - Remove {summary['constant_features']} constant features\")\n",
        "if summary['near_constant_features'] > 0:\n",
        "    print(f\"   - Consider removing {summary['near_constant_features']} near-constant features\")\n",
        "if summary['perfect_corr_features'] > 0:\n",
        "    print(f\"   - Remove {summary['perfect_corr_features']} features with perfect correlation (potential leakage)\")\n",
        "\n",
        "print(\"\\n2. FEATURE ENGINEERING:\")\n",
        "if summary['high_missing_features'] > 0:\n",
        "    print(f\"   - Implement robust missing value handling for {summary['high_missing_features']} features\")\n",
        "if summary['extreme_outlier_features'] > 0:\n",
        "    print(f\"   - Apply outlier treatment to {summary['extreme_outlier_features']} features\")\n",
        "if summary['high_corr_pairs'] > 0:\n",
        "    print(f\"   - Consider feature selection to reduce {summary['high_corr_pairs']} high correlation pairs\")\n",
        "\n",
        "print(\"\\n3. DATA QUALITY:\")\n",
        "if summary['infinite_value_features'] > 0:\n",
        "    print(f\"   - Handle infinite values in {summary['infinite_value_features']} features\")\n",
        "if summary['suspicious_range_features'] > 0:\n",
        "    print(f\"   - Investigate suspicious ranges in {summary['suspicious_range_features']} features\")\n",
        "\n",
        "print(\"\\n4. MODEL CONSIDERATIONS:\")\n",
        "if summary['significant_dist_diff'] > 0:\n",
        "    print(f\"   - Monitor {summary['significant_dist_diff']} features with distribution shifts\")\n",
        "if summary['high_psi_features'] > 0:\n",
        "    print(f\"   - Re-evaluate {summary['high_psi_features']} features with high PSI\")\n",
        "if summary['high_vif_features'] > 0:\n",
        "    print(f\"   - Address multicollinearity in {summary['high_vif_features']} features\")\n",
        "\n",
        "# Save summary\n",
        "import json\n",
        "with open('../Data/processed/feature_checks_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ… Feature checks summary saved to: ../Data/processed/feature_checks_summary.json\")\n",
        "print(\"\\nðŸŽ¯ Next steps: Apply recommendations before model training!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
