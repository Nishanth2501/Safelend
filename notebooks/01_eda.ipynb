{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SafeLend - Exploratory Data Analysis\n",
        "\n",
        "This notebook contains exploratory data analysis for the SafeLend credit risk prediction project.\n",
        "\n",
        "## Overview\n",
        "- Data exploration and visualization\n",
        "- Missing value analysis\n",
        "- Feature distribution analysis\n",
        "- Correlation analysis\n",
        "- Target variable analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "application_train = pd.read_csv('../Data/raw/application_train.csv')\n",
        "application_test = pd.read_csv('../Data/raw/application_test.csv')\n",
        "\n",
        "print(f\"Training data shape: {application_train.shape}\")\n",
        "print(f\"Test data shape: {application_test.shape}\")\n",
        "\n",
        "# Basic data info\n",
        "print(\"\\nTraining data info:\")\n",
        "print(application_train.info())\n",
        "print(\"\\nTest data info:\")\n",
        "print(application_test.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset info analysis\n",
        "print(\"=== DATASET OVERVIEW ===\")\n",
        "print(f\"Training set: {application_train.shape[0]:,} applicants, {application_train.shape[1]} features\")\n",
        "print(f\"Test set: {application_test.shape[0]:,} applicants, {application_test.shape[1]} features\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n=== MISSING VALUES ANALYSIS ===\")\n",
        "train_missing = application_train.isnull().sum()\n",
        "test_missing = application_test.isnull().sum()\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Train_Missing': train_missing,\n",
        "    'Test_Missing': test_missing,\n",
        "    'Train_Percent': (train_missing / len(application_train) * 100).round(2),\n",
        "    'Test_Percent': (test_missing / len(application_test) * 100).round(2)\n",
        "})\n",
        "\n",
        "# Show columns with highest missing values\n",
        "missing_df = missing_df[missing_df['Train_Missing'] > 0].sort_values('Train_Percent', ascending=False)\n",
        "print(\"Top 15 columns with missing values:\")\n",
        "print(missing_df.head(15))\n",
        "\n",
        "# Data types analysis\n",
        "print(\"\\n=== DATA TYPES ANALYSIS ===\")\n",
        "print(\"Training data types:\")\n",
        "print(application_train.dtypes.value_counts())\n",
        "print(\"\\nTest data types:\")\n",
        "print(application_test.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target variable analysis\n",
        "print(\"=== TARGET VARIABLE ANALYSIS ===\")\n",
        "\n",
        "# Target distribution\n",
        "target_counts = application_train['TARGET'].value_counts()\n",
        "target_pct = application_train['TARGET'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Target distribution:\")\n",
        "for value, count in target_counts.items():\n",
        "    pct = target_pct[value]\n",
        "    label = \"Default\" if value == 1 else \"Repay\"\n",
        "    print(f\"  {label}: {count:,} ({pct:.2f}%)\")\n",
        "\n",
        "# Visualize target distribution\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "target_counts.plot(kind='bar', color=['green', 'red'])\n",
        "plt.title('Target Distribution (Count)')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks([0, 1], ['Repay (0)', 'Default (1)'], rotation=0)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "target_pct.plot(kind='bar', color=['green', 'red'])\n",
        "plt.title('Target Distribution (Percentage)')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xticks([0, 1], ['Repay (0)', 'Default (1)'], rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Class imbalance analysis\n",
        "print(f\"\\nClass imbalance ratio: {target_counts[1] / target_counts[0]:.3f}\")\n",
        "print(f\"Minority class percentage: {target_pct[1]:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values analysis\n",
        "print(\"=== MISSING VALUES VISUALIZATION ===\")\n",
        "\n",
        "# Create missing values heatmap for top missing columns\n",
        "top_missing_cols = missing_df.head(20).index\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "# Training set missing values\n",
        "plt.subplot(2, 1, 1)\n",
        "train_missing_subset = application_train[top_missing_cols].isnull().sum()\n",
        "train_missing_subset.plot(kind='bar', color='skyblue')\n",
        "plt.title('Missing Values in Training Set (Top 20 Columns)')\n",
        "plt.ylabel('Missing Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Test set missing values\n",
        "plt.subplot(2, 1, 2)\n",
        "test_missing_subset = application_test[top_missing_cols].isnull().sum()\n",
        "test_missing_subset.plot(kind='bar', color='lightcoral')\n",
        "plt.title('Missing Values in Test Set (Top 20 Columns)')\n",
        "plt.ylabel('Missing Count')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Missing values patterns\n",
        "print(\"\\nMissing values patterns:\")\n",
        "print(f\"Columns with >50% missing: {(missing_df['Train_Percent'] > 50).sum()}\")\n",
        "print(f\"Columns with >25% missing: {(missing_df['Train_Percent'] > 25).sum()}\")\n",
        "print(f\"Columns with >10% missing: {(missing_df['Train_Percent'] > 10).sum()}\")\n",
        "\n",
        "# Columns with no missing values\n",
        "no_missing = (application_train.isnull().sum() == 0).sum()\n",
        "print(f\"Columns with no missing values: {no_missing}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature distribution analysis\n",
        "print(\"=== FEATURE DISTRIBUTION ANALYSIS ===\")\n",
        "\n",
        "# Select key numerical features for analysis\n",
        "key_features = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', \n",
        "                'DAYS_BIRTH', 'DAYS_EMPLOYED', 'CNT_FAM_MEMBERS', 'CNT_CHILDREN']\n",
        "\n",
        "# Filter features that exist in the dataset\n",
        "existing_features = [f for f in key_features if f in application_train.columns]\n",
        "print(f\"Analyzing {len(existing_features)} key features: {existing_features}\")\n",
        "\n",
        "# Distribution plots\n",
        "n_features = len(existing_features)\n",
        "n_cols = 3\n",
        "n_rows = (n_features + n_cols - 1) // n_cols\n",
        "\n",
        "plt.figure(figsize=(15, 5 * n_rows))\n",
        "\n",
        "for i, feature in enumerate(existing_features):\n",
        "    plt.subplot(n_rows, n_cols, i + 1)\n",
        "    \n",
        "    # Plot distribution for each target class\n",
        "    repay_data = application_train[application_train['TARGET'] == 0][feature].dropna()\n",
        "    default_data = application_train[application_train['TARGET'] == 1][feature].dropna()\n",
        "    \n",
        "    plt.hist(repay_data, bins=50, alpha=0.7, label='Repay', color='green', density=True)\n",
        "    plt.hist(default_data, bins=50, alpha=0.7, label='Default', color='red', density=True)\n",
        "    \n",
        "    plt.title(f'{feature} Distribution')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.yscale('log')  # Log scale for better visualization\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical summary\n",
        "print(\"\\nStatistical Summary of Key Features:\")\n",
        "for feature in existing_features:\n",
        "    print(f\"\\n{feature}:\")\n",
        "    print(f\"  Mean: {application_train[feature].mean():.2f}\")\n",
        "    print(f\"  Median: {application_train[feature].median():.2f}\")\n",
        "    print(f\"  Std: {application_train[feature].std():.2f}\")\n",
        "    print(f\"  Min: {application_train[feature].min():.2f}\")\n",
        "    print(f\"  Max: {application_train[feature].max():.2f}\")\n",
        "    print(f\"  Missing: {application_train[feature].isnull().sum():,} ({application_train[feature].isnull().mean()*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n",
        "print(\"=== CORRELATION ANALYSIS ===\")\n",
        "\n",
        "# Select numerical features for correlation analysis\n",
        "numerical_features = application_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numerical_features = [f for f in numerical_features if f not in ['SK_ID_CURR', 'TARGET']]\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = application_train[numerical_features + ['TARGET']].corr()\n",
        "\n",
        "# Correlation with target\n",
        "target_corr = correlation_matrix['TARGET'].drop('TARGET').sort_values(key=abs, ascending=False)\n",
        "print(\"Top 20 features most correlated with TARGET:\")\n",
        "print(target_corr.head(20))\n",
        "\n",
        "# Visualize correlation matrix for top features\n",
        "top_features = target_corr.head(15).index.tolist() + ['TARGET']\n",
        "corr_subset = correlation_matrix.loc[top_features, top_features]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_subset, annot=True, cmap='coolwarm', center=0, \n",
        "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
        "plt.title('Correlation Matrix - Top Features with Target')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance by correlation\n",
        "plt.figure(figsize=(12, 8))\n",
        "target_corr.head(20).plot(kind='barh', color=['red' if x > 0 else 'blue' for x in target_corr.head(20)])\n",
        "plt.title('Top 20 Features by Correlation with Target')\n",
        "plt.xlabel('Correlation with Target')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical features analysis\n",
        "print(\"=== CATEGORICAL FEATURES ANALYSIS ===\")\n",
        "\n",
        "# Get categorical features\n",
        "categorical_features = application_train.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Found {len(categorical_features)} categorical features:\")\n",
        "print(categorical_features)\n",
        "\n",
        "# Analyze each categorical feature\n",
        "for feature in categorical_features[:10]:  # Analyze first 10 categorical features\n",
        "    print(f\"\\n{feature}:\")\n",
        "    value_counts = application_train[feature].value_counts()\n",
        "    print(f\"  Unique values: {application_train[feature].nunique()}\")\n",
        "    print(f\"  Most common: {value_counts.head(3).to_dict()}\")\n",
        "    \n",
        "    # Default rate by category\n",
        "    if application_train[feature].nunique() <= 20:  # Only for features with reasonable number of categories\n",
        "        default_rates = application_train.groupby(feature)['TARGET'].mean().sort_values(ascending=False)\n",
        "        print(f\"  Default rates: {default_rates.head(5).to_dict()}\")\n",
        "\n",
        "# Visualize categorical features with reasonable number of categories\n",
        "categorical_to_plot = [f for f in categorical_features if application_train[f].nunique() <= 10 and application_train[f].nunique() > 1]\n",
        "n_cat = len(categorical_to_plot)\n",
        "\n",
        "if n_cat > 0:\n",
        "    n_cols = 2\n",
        "    n_rows = (n_cat + n_cols - 1) // n_cols\n",
        "    \n",
        "    plt.figure(figsize=(15, 5 * n_rows))\n",
        "    \n",
        "    for i, feature in enumerate(categorical_to_plot):\n",
        "        plt.subplot(n_rows, n_cols, i + 1)\n",
        "        \n",
        "        # Create crosstab\n",
        "        crosstab = pd.crosstab(application_train[feature], application_train['TARGET'], normalize='index')\n",
        "        crosstab.plot(kind='bar', stacked=True, ax=plt.gca(), color=['green', 'red'])\n",
        "        plt.title(f'{feature} vs Target')\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel('Proportion')\n",
        "        plt.legend(['Repay', 'Default'])\n",
        "        plt.xticks(rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary and insights\n",
        "print(\"=== EDA SUMMARY AND INSIGHTS ===\")\n",
        "\n",
        "print(\"1. DATASET OVERVIEW:\")\n",
        "print(f\"   - Training set: {application_train.shape[0]:,} applicants with {application_train.shape[1]} features\")\n",
        "print(f\"   - Test set: {application_test.shape[0]:,} applicants with {application_test.shape[1]} features\")\n",
        "print(f\"   - Class imbalance: {target_counts[1] / target_counts[0]:.3f} (default/repay ratio)\")\n",
        "\n",
        "print(\"\\n2. TARGET VARIABLE:\")\n",
        "print(f\"   - Default rate: {target_pct[1]:.2f}%\")\n",
        "print(f\"   - This is a highly imbalanced dataset requiring special handling\")\n",
        "\n",
        "print(\"\\n3. MISSING VALUES:\")\n",
        "print(f\"   - Columns with >50% missing: {(missing_df['Train_Percent'] > 50).sum()}\")\n",
        "print(f\"   - Columns with >25% missing: {(missing_df['Train_Percent'] > 25).sum()}\")\n",
        "print(f\"   - Columns with no missing: {no_missing}\")\n",
        "\n",
        "print(\"\\n4. FEATURE TYPES:\")\n",
        "print(f\"   - Numerical features: {len(numerical_features)}\")\n",
        "print(f\"   - Categorical features: {len(categorical_features)}\")\n",
        "\n",
        "print(\"\\n5. KEY INSIGHTS:\")\n",
        "print(\"   - Strong class imbalance requires careful model evaluation\")\n",
        "print(\"   - Many features have high missing value rates\")\n",
        "print(\"   - Feature engineering will be crucial for model performance\")\n",
        "print(\"   - Correlation analysis shows some features are more predictive than others\")\n",
        "\n",
        "print(\"\\n6. RECOMMENDATIONS:\")\n",
        "print(\"   - Use stratified sampling for model validation\")\n",
        "print(\"   - Implement robust missing value handling\")\n",
        "print(\"   - Focus on feature engineering and selection\")\n",
        "print(\"   - Consider ensemble methods to handle class imbalance\")\n",
        "print(\"   - Use appropriate evaluation metrics (ROC-AUC, PR-AUC)\")\n",
        "\n",
        "# Save summary to file\n",
        "summary_data = {\n",
        "    'dataset_size': {\n",
        "        'train_samples': application_train.shape[0],\n",
        "        'test_samples': application_test.shape[0],\n",
        "        'total_features': application_train.shape[1]\n",
        "    },\n",
        "    'target_distribution': {\n",
        "        'default_rate': float(target_pct[1]),\n",
        "        'class_imbalance_ratio': float(target_counts[1] / target_counts[0])\n",
        "    },\n",
        "    'missing_values': {\n",
        "        'high_missing_columns': int((missing_df['Train_Percent'] > 50).sum()),\n",
        "        'total_missing_columns': int((missing_df['Train_Percent'] > 0).sum())\n",
        "    },\n",
        "    'feature_types': {\n",
        "        'numerical': len(numerical_features),\n",
        "        'categorical': len(categorical_features)\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../Data/processed/eda_summary.json', 'w') as f:\n",
        "    json.dump(summary_data, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ EDA Summary saved to: ../Data/processed/eda_summary.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numerical features analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical features analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
